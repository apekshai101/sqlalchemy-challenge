# sqlalchemy-challenge
This challenge uses Python and SQLAlchemy to do a basic climate analysis and data exploration climate database. 


## Data Source
•    Data for this dataset was generated by 2024 edX Boot Camps LLC with references Menne, M.J., I. Durre, R.S. Vose, B.E. Gleason, and T.G. Houston, 2012: An overview of the Global Historical Climatology Network-Daily Database. Journal of Atmospheric and Oceanic Technology, 29, 897-910, https://journals.ametsoc.org/view/journals/atot/29/7/jtech-d-11-00103_1.xml
•    These files (climate_starter.ipynb and hawaii.sqlite) were given to complete climate analysis and data exploration.

## Instructions:
Part 1: Analyze and Explore the Climate Data
Before analyzing the data, 
•    SQLAlchemy create_engine() function was used  to connect to hawaii.sqlite database. 
•    SQLAlchemy automap_base() function was used to reflect tables into classes, and then references were saved to the classes named station and measurement. 
•    Python library was linked to the database by creating a SQLAlchemy session.
Precipitation Analysis
1.    The most recent date in the dataset was (2017-08-23)
2.    Using recent date, previous 12 months of precipitation data was obtained by querying the previous 12 months of data.
3.    A query was performed to retrieve the data and precipitation scores.
4.    The query results were loaded into a Pandas DataFrame explicitly setting the column names.
5.    The DataFrame values were sorted by "date".
6.    The results were plotted by using the DataFrame plot method
7.    Pandas was used to print the summary statistics for the precipitation data.
Station Analysis
1.    A query was created to calculate the total number of stations in the dataset.
2.    A query was created to find the most-active stations (that is, the stations that have the most rows).
3.    A query  was created that calculates the lowest, highest, and average temperatures that filters on the most-active station id found in the previous query.
4.    A  query was done to get the previous 12 months of temperature observation (TOBS) data and a histogram with bins=12 were plotted.
Part 2: Design Your Climate App
 a Flask API was designed based on the queries mentioned above. To do so, Flask is used to create routes as follows:
1.    /
At the start, list of all the available routes were mentioned.

2.    /api/v1.0/precipitation route - the query results were converted from precipitation analysis (only the last 12 months of data) to a dictionary using date as the key and prcp as the value and return the JSON representation of dictionary.

3.    /api/v1.0/stations route - Return a JSON list of stations from the dataset.


4.    /api/v1.0/tobs route - Query the dates and temperature observations of the most-active station for the previous year of data and return a JSON list of temperature observations for the previous year.

5.    /api/v1.0/<start> and /api/v1.0/<start>/<end> route - Return a JSON list of the minimum temperature, the average temperature, and the maximum temperature for a specified start or start-end range.


6.    For a specified start, TMIN, TAVG, and TMAX for all the dates greater than or equal to the start date was calculated.

7.    For a specified start date and end date, TMIN, TAVG, and TMAX for the dates from the start date to the end date, inclusive was calculated.


### Prerequisites
•    Tools need to have installed before project:
•    Jupyter notebook
•    Matplotlib
•    Pandas
•    Python toolkit and object relational mapper
•    Sqlalchemy
•    flask
•    GitHub
•    Local git repository sqlalchemy-challenge was created , cloned and changes were pushed to main branch.


